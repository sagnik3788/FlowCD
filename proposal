main arch will follow argo cd
like main three components
1. flowcd server
2. flowcd controller
3. flowctl for interaction(+tui)

- first we will implement the crd and the controller as this the main block

- in the crd we will define the template we want to follow like source means repo url branch ns,
destination for cluster , and sync stagies same as like pipecd quick-sync, pipeline-sync and custom-sync

- For creating the controller we will define the types we are using crd

- How the controller will work? why we need this?
- so whenever anyone create a custom resource based on our template(crd) we need to monitor it and apply on our cluster right, means in the custom resourse a user will give the repo url , branch path of the manifest files what he want to deploy and where means in which namespace of his k8s cluster

For build this logic we need the controller --> main task will be a starting the controller  then start a reconcile logic which will clone/pull the repo changes and apply on the cluster thats how all the controller of all gitops tools works like pipcd or argocd, just the main arch is different

- How to build the controller? as we have created the types based on crd now we can implement basic controller logic with reconcile which only compile and running with logging , next we will implement git logic and k8s and caching repo etc

- Buidling the reconcile logic is quite easy we just need to create a empty object say template of our cr, then get request to the k8s api next log everything like repo url, branch etc
one thing i jsut know like to crud in k8s api we need to setup scheme  which actually convert json to go structs or vice versa

- while building i came into an error

'cannot use &FlowCD{} as "runtime".Object value in argument to scheme.AddKnownTypes:
*FlowCD does not implement "runtime.Object" (missing method DeepCopyObject)'

so our custum type(2) missing DeepCopyObject() method , so instead manually we gen kubebuilder files, this method help to prevent race cond like sinarious like staus= sync or status = failure at same time


- now we have applied decent controller but , what if someone chnage something on the cluster manually using kubectl so we need to implement livestate and compare with desired state means on the git and update the status like synced or out of synced
next features:- livestate and drift detection

developed the livestate which can list all the resources using existing k8s client from client.go and the list deployments, services and log them for test , while doing this faced a new issue like i cahnge directly manifest repo in the cache and same in the git , now there is a conflict issue and error
i have checked how argo cd solve this:- they run cache on repo pod/controller (we dont have this users done have access)
plus in argo if there is any confilct in pull they jsut delete the full cache and reclone 
So i will also follow this if conflict delete cache and reclone 


applied drift detcion fxn where we count the extra in live and missing means less in live comapred to git state , so we check the specs of manifest and apply if chnage something 
but here we are facing an issue like when all resources of live and git is same but the internal specs are diff then we do compare in specs in that fxn we fully compare the whole hjson of two specs due to that 
lot of things are getting involved from the live json which is not in git specs json, so in argo they compare only desired with the same type available in the live specs (dont touch live specs)
Will implement hsi same for mine